# APGD-Accelerated-Proximal-Gradient-Descent-Method
This is a python implementation of Accelerated Proximal Gradient Descent method.



Different from Nesterov's Accelerated Gradient, this chooses a different approach to update $y^{k+1}$, but sharing the same convergence rate $O(1/k^2)$.
